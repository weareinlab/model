{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features-Extract.py\n",
    "from __future__ import print_function\n",
    "from keras.applications.inception_v3 import InceptionV3, conv2d_bn\n",
    "from keras.applications.densenet  import DenseNet201\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras import optimizers\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import h5py\n",
    "\n",
    "conv_base = InceptionV3(weights='imagenet', include_top=False)\n",
    "train_dir ='D:/OCT/dataset-size-299/train/'\n",
    "validation_dir = 'D:/OCT/dataset-size-299/val/'\n",
    "\n",
    "def extract_features(file_name, directory, key, sample_count, target_size, batch_size, class_mode='categorical'):\n",
    "    h5_file = h5py.File(file_name, 'w')\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    generator = datagen.flow_from_directory(directory, target_size=target_size,\n",
    "        batch_size=batch_size, class_mode=class_mode)\n",
    "    samples_processed = 0\n",
    "    batch_number = 0\n",
    "    if sample_count == 'all':\n",
    "        sample_count = generator.n\n",
    "    print_size = True\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        if print_size == True:\n",
    "            print_size = False\n",
    "            print('Features shape', features_batch.shape)\n",
    "        samples_processed += inputs_batch.shape[0]\n",
    "        h5_file.create_dataset('features-'+ str(batch_number), data=features_batch)\n",
    "        h5_file.create_dataset('labels-'+str(batch_number), data=labels_batch)\n",
    "        batch_number = batch_number + 1\n",
    "        print(\"Batch:%d Sample:%d\\r\" % (batch_number,samples_processed), end=\"\")\n",
    "        if samples_processed >= sample_count:\n",
    "            break\n",
    "    h5_file.create_dataset('batches', data=batch_number)\n",
    "    h5_file.close()\n",
    "    return\n",
    "\n",
    "extract_features('train.h5', train_dir, key='train', sample_count='all', batch_size=100, target_size=(299,299))\n",
    "\n",
    "extract_features('val.h5', validation_dir, key='val', sample_count='all', batch_size=100, target_size=(299,299))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features-Train.py\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import conv2d_bn\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def features_from_file(path, ctx):\n",
    "    h5f = h5py.File(path, 'r')\n",
    "    batch_count = h5f['batches'][()]\n",
    "    print(ctx, 'batches:', batch_count)       \n",
    "    def generator():\n",
    "        while True:\n",
    "            for batch_id in range(0, batch_count):\n",
    "                X = h5f['features-' + str(batch_id)]\n",
    "                y = h5f['labels-' + str(batch_id)]\n",
    "                yield X, y      \n",
    "    return batch_count, generator()\n",
    "\n",
    "train_steps_per_epoch, train_generator = features_from_file('D:/OCT/inceptionv3/train.h5', 'train')\n",
    "validation_steps, validation_data = features_from_file('D:/OCT/inceptionv3/val.h5', 'val')\n",
    "np.random.seed(7)\n",
    "inputs = Input(shape=(8, 8, 2048))\n",
    "x = conv2d_bn(inputs, 64, 1, 1)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "outputs  = Dense(4, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [ ModelCheckpoint('D:/OCT/inceptionv3/model.features.{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max', period=1),\n",
    "              ReduceLROnPlateau(monitor='val_loss', verbose=1, factor=0.5, patience=5, min_lr=0.00005)]\n",
    "\n",
    "history = model.fit_generator(generator=train_generator, steps_per_epoch=train_steps_per_epoch, validation_data=validation_data, validation_steps=validation_steps, epochs=100, callbacks=callbacks)\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(epochs, acc, 'b', color='red' , label='Training Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(epochs, val_acc, 'b', color='red', label='Validation Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(epochs, loss, 'b',color='red', label='Training Loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(epochs, val_loss, 'b', color='red', label='Validation Loss')\n",
    "    plt.title('Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return acc, val_acc, loss, val_loss\n",
    "\n",
    "acc, val_acc, loss, val_loss = plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Evaluate.py\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import colors\n",
    "import requests\n",
    "\n",
    "K.set_learning_phase(0) \n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "model = load_model('model.features.57-0.99.hdf5')\n",
    "print(model.summary())\n",
    "\n",
    "classes = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
    "def preprocess_input(x):\n",
    "    x = img_to_array(x) / 255.\n",
    "    return np.expand_dims(x, axis=0) \n",
    "\n",
    "def predict_from_image_path(image_path):\n",
    "    return predict_image(load_img(image_path, target_size=(299, 299)))\n",
    "\n",
    "def predict_from_image_url(image_url):\n",
    "    res = requests.get(image_url)\n",
    "    im = Image.open(BytesIO(res.content))\n",
    "    return predict_from_image_path(im.fp)\n",
    "    \n",
    "def predict_image(im):\n",
    "    x = preprocess_input(im)\n",
    "    x = base_model.predict(x)\n",
    "    pred = np.argmax(model.predict(x))\n",
    "    return pred, classes[pred]\n",
    "\n",
    "def grad_CAM(image_path):\n",
    "    im = load_img(image_path, target_size=(299,299))\n",
    "    x = preprocess_input(im)\n",
    "    x = base_model.predict(x)\n",
    "    pred = model.predict(x)\n",
    "    index = np.argmax(pred)\n",
    "    class_output = model.output[:, index]\n",
    "    last_conv_layer = model.get_layer('conv2d')\n",
    "    nmb_channels = last_conv_layer.output.shape[3]\n",
    "    grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    iterate = K.function(model.inputs, [pooled_grads, last_conv_layer.output[0]])\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "    for i in range(nmb_channels):\n",
    "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    img = cv2.imread(image_path)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.5 + img \n",
    "    plt.figure(figsize=(24,12))\n",
    "    cv2.imwrite('./tmp.jpeg', superimposed_img)\n",
    "    plt.imshow(mpimg.imread('./tmp.jpeg'))\n",
    "    plt.title(image_path)\n",
    "    plt.show()\n",
    "\n",
    "for i, c in enumerate(classes):\n",
    "    folder = 'D:/OCT/dataset-size-299/test/' + c + '/'\n",
    "    count = 1\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.jpeg') == True:\n",
    "            image_path = folder + file\n",
    "            p, class_name = predict_from_image_path(image_path)\n",
    "            if p == i:\n",
    "                print(file, p, class_name)\n",
    "            else:\n",
    "                print(file, p, class_name, '**INCORRECT PREDICTION**')\n",
    "                grad_CAM(image_path)\n",
    "        count = count +1\n",
    "        if count == 100:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
