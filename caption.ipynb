{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "def build_cnn_encoder():\n",
    "    model = InceptionV3(weights='imagenet')\n",
    "    model = Model(inputs=model.input, outputs=model.layers[-2].output)  # Remove the top layer\n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(vocab_size, max_length):\n",
    "    # Image feature input\n",
    "    image_input = Input(shape=(2048,))\n",
    "    image_features = Dropout(0.5)(image_input)\n",
    "    image_features = Dense(256, activation='relu')(image_features)\n",
    "    \n",
    "    # Sequence input for the text\n",
    "    seq_input = Input(shape=(max_length,))\n",
    "    seq_features = Embedding(vocab_size, 256, mask_zero=True)(seq_input)\n",
    "    seq_features = Dropout(0.5)(seq_features)\n",
    "    seq_features = LSTM(256)(seq_features)\n",
    "    \n",
    "    # Combine image features and text features\n",
    "    decoder = add([image_features, seq_features])\n",
    "    decoder = Dense(256, activation='relu')(decoder)\n",
    "    output = Dense(vocab_size, activation='softmax')(decoder)\n",
    "    \n",
    "    # Define the final model\n",
    "    model = Model(inputs=[image_input, seq_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "vocab_size = 5000  # Size of the vocabulary\n",
    "max_length = 20    # Maximum length of captions\n",
    "decoder_model = build_decoder(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(image_path, model):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    features = model.predict(img)\n",
    "    return features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_caption(image_features, tokenizer, max_length, temperature=1.0):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        # Tokenize and pad the input text\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        \n",
    "        # Predict the probability distribution over the next word\n",
    "        yhat = decoder_model.predict([image_features.reshape(1, 2048), sequence], verbose=0)[0]\n",
    "        \n",
    "        # Apply temperature to adjust the probability distribution\n",
    "        yhat = np.log(yhat + 1e-8) / temperature  # Log scaling\n",
    "        yhat = np.exp(yhat) / np.sum(np.exp(yhat))  # Softmax\n",
    "\n",
    "        # Sample the next word index from the adjusted probability distribution\n",
    "        next_word_index = np.random.choice(len(yhat), p=yhat)\n",
    "\n",
    "        # Convert index to word and handle unknowns\n",
    "        word = tokenizer.index_word.get(next_word_index, \"unknown\")\n",
    "\n",
    "        # If the word is \"endseq\", stop generating\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "        \n",
    "        # Append the word to the generated caption\n",
    "        in_text += ' ' + word\n",
    "\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Generated Caption: startseq unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown unknown\n"
     ]
    }
   ],
   "source": [
    "image_path = 'church.jpg'  # Replace with the path to an image\n",
    "image_features = extract_image_features(image_path, cnn_model)\n",
    "caption = generate_caption(image_features, tokenizer, max_length)\n",
    "print(\"Generated Caption:\", caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
