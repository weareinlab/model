{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Values:\n",
      "[[27.70775002 32.89750102 38.66389102 37.78099102 45.79999102]\n",
      " [41.89750102 47.66389102 54.07099102 53.08999102 61.99999102]\n",
      " [47.66389102 54.07099102 61.18999102 70.09999102 79.99999102]\n",
      " [62.17099102 70.18999102 79.09999102 88.99999102 99.99999102]\n",
      " [70.18999102 79.09999102 88.99999102 99.99999102 99.99999102]]\n",
      "\n",
      "Optimal Policy:\n",
      "[[1 1 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [3 3 1 1 1]\n",
      " [1 1 1 1 1]\n",
      " [3 3 3 3 0]]\n"
     ]
    }
   ],
   "source": [
    "# One Grid World\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BridgeGridEnv:\n",
    "    def __init__(self, grid_size=(5, 5), bridge_location=(2, 0), bridge_length=3, goal_state=(4, 4), fall_penalty=-10, goal_reward=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.bridge_location = bridge_location\n",
    "        self.bridge_length = bridge_length\n",
    "        self.goal_state = goal_state\n",
    "        self.fall_penalty = fall_penalty\n",
    "        self.goal_reward = goal_reward\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.n_states = grid_size[0] * grid_size[1]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.transitions, self.rewards = self.build_environment()\n",
    "    \n",
    "    def build_environment(self):\n",
    "        transition_prob = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
    "        rewards = np.full((self.n_states, self.n_actions), -1)\n",
    "        def state_to_index(row, col):\n",
    "            return row * self.grid_size[1] + col\n",
    "        def is_off_bridge(row, col):\n",
    "            return (row != self.bridge_location[0] or col >= self.bridge_location[1] + self.bridge_length) and row < self.grid_size[0] - 1\n",
    "        for row in range(self.grid_size[0]):\n",
    "            for col in range(self.grid_size[1]):\n",
    "                state = state_to_index(row, col)\n",
    "                for action_idx, action in enumerate(self.actions):\n",
    "                    if (row, col) == self.goal_state:\n",
    "                        rewards[state, action_idx] = self.goal_reward\n",
    "                        transition_prob[state, action_idx, state] = 1.0\n",
    "                        continue\n",
    "                    next_row, next_col = row, col\n",
    "                    if action == 'up' and row > 0:\n",
    "                        next_row = row - 1\n",
    "                    elif action == 'down' and row < self.grid_size[0] - 1:\n",
    "                        next_row = row + 1\n",
    "                    elif action == 'left' and col > 0:\n",
    "                        next_col = col - 1\n",
    "                    elif action == 'right' and col < self.grid_size[1] - 1:\n",
    "                        next_col = col + 1\n",
    "                    next_state = state_to_index(next_row, next_col)\n",
    "                    if (next_row, next_col) == self.goal_state:\n",
    "                        rewards[state, action_idx] = self.goal_reward\n",
    "                    elif is_off_bridge(next_row, next_col):\n",
    "                        rewards[state, action_idx] = self.fall_penalty\n",
    "                    transition_prob[state, action_idx, next_state] = 1.0\n",
    "        return transition_prob, rewards\n",
    "\n",
    "    def state_index_to_coordinates(self, state_index):\n",
    "        row = state_index // self.grid_size[1]\n",
    "        col = state_index % self.grid_size[1]\n",
    "        return row, col\n",
    "\n",
    "\n",
    "def value_iteration_bridge(env, gamma=0.9, theta=1e-6):\n",
    "    V = np.zeros(env.n_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.n_states):\n",
    "            v = V[s]\n",
    "            Q_sa = np.zeros(env.n_actions)\n",
    "            for a in range(env.n_actions):\n",
    "                Q_sa[a] = sum([env.transitions[s, a, s_prime] * (env.rewards[s, a] + gamma * V[s_prime]) for s_prime in range(env.n_states)])\n",
    "            V[s] = max(Q_sa)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros(env.n_states, dtype=int)\n",
    "    for s in range(env.n_states):\n",
    "        Q_sa = np.zeros(env.n_actions)\n",
    "        for a in range(env.n_actions):\n",
    "            Q_sa[a] = sum([env.transitions[s, a, s_prime] * (env.rewards[s, a] + gamma * V[s_prime])\n",
    "                           for s_prime in range(env.n_states)])\n",
    "        policy[s] = np.argmax(Q_sa)\n",
    "    return V, policy\n",
    "\n",
    "env = BridgeGridEnv()\n",
    "V, policy = value_iteration_bridge(env)\n",
    "print(\"Optimal Values:\")\n",
    "print(V.reshape(env.grid_size))\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(policy.reshape(env.grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 complete\n",
      "Episode 100 complete\n",
      "Episode 200 complete\n",
      "Episode 300 complete\n",
      "Episode 400 complete\n",
      "Episode 500 complete\n",
      "Episode 600 complete\n",
      "Episode 700 complete\n",
      "Episode 800 complete\n",
      "Episode 900 complete\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: right, Reward: -0.1\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: up, Reward: -0.1\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: up, Reward: 0\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: up, Reward: -0.1\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: down, Reward: 0\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: right, Reward: 0\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: right, Reward: 0\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: up, Reward: -0.1\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: up, Reward: -0.1\n",
      "[[0.  0.  0.  0.  1. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.3 0.3 0.3 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.5 0.  0.  0.  0. ]]\n",
      "Action: right, Reward: 1\n"
     ]
    }
   ],
   "source": [
    "# Two Grid World\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class BridgeGrid:\n",
    "    def __init__(self):\n",
    "        self.grid = np.zeros((5, 5))\n",
    "        self.goal = (0, 4)\n",
    "        self.start = (4, 0)\n",
    "        self.bridge = [(2, 1), (2, 2), (2, 3)]\n",
    "        self.state = self.start\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        row, col = self.state\n",
    "        if action == 'up':\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 'down':\n",
    "            row = min(4, row + 1)\n",
    "        elif action == 'left':\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 'right':\n",
    "            col = min(4, col + 1)\n",
    "        next_state = (row, col)\n",
    "        if next_state == self.goal:\n",
    "            reward = 1 \n",
    "            done = True\n",
    "        elif next_state in self.bridge:\n",
    "            reward = 0\n",
    "            done = False\n",
    "        elif row == 2 and col not in [1, 2, 3]:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.1 \n",
    "            done = False\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        grid_copy = np.copy(self.grid)\n",
    "        grid_copy[self.goal] = 1\n",
    "        grid_copy[self.start] = 0.5\n",
    "        for bridge_part in self.bridge:\n",
    "            grid_copy[bridge_part] = 0.3\n",
    "        print(grid_copy)\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        max_next_q_value = max([self.get_q_value(next_state, a) for a in self.env.actions])\n",
    "        current_q_value = self.get_q_value(state, action)\n",
    "        new_q_value = current_q_value + self.alpha * (reward + self.gamma * max_next_q_value - current_q_value)\n",
    "        self.q_table[(state, action)] = new_q_value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.env.actions)\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, action) for action in self.env.actions]\n",
    "            max_q = max(q_values)\n",
    "            max_actions = [action for action, q_value in zip(self.env.actions, q_values) if q_value == max_q]\n",
    "            return random.choice(max_actions)\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.update_q_value(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "            if episode % 100 == 0:\n",
    "                print(f\"Episode {episode} complete\")\n",
    "\n",
    "env = BridgeGrid()\n",
    "agent = QLearningAgent(env)\n",
    "agent.train(episodes=1000)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.choose_action(state)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    state = next_state\n",
    "    env.render()\n",
    "    print(f\"Action: {action}, Reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
