{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-zW_Wm2BXSp",
    "outputId": "50a253ea-1536-4903-bdeb-0eea54ba90bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 17026, done.\u001b[K\n",
      "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 17026 (delta 1), reused 0 (delta 0), pack-reused 17022 (from 1)\u001b[K\n",
      "Receiving objects: 100% (17026/17026), 15.64 MiB | 8.24 MiB/s, done.\n",
      "Resolving deltas: 100% (11695/11695), done.\n",
      "/content/yolov5\n",
      "HEAD is now at 064365d8 Update parse_opt() in export.py to work as in train.py (#10789)\n"
     ]
    }
   ],
   "source": [
    "# clone YOLOv5 repository\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "%cd yolov5\n",
    "!git reset --hard 064365d8683fd002e9ad789c1e91fa3d021b44f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjmfoh4LBcqQ",
    "outputId": "286b53ce-c49d-41dc-9da5-cefeb05e0c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.4/1.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hSetup complete. Using torch 2.5.0+cu121 CPU\n"
     ]
    }
   ],
   "source": [
    "# install dependencies as necessary\n",
    "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "from utils.downloads import attempt_download  # to download models/datasets\n",
    "\n",
    "# clear_output()\n",
    "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBwQyIEyCOeS",
    "outputId": "4eee4f02-0d46-48c5-d5b4-3ce48c683e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/80.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in object-1 to yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3557/3557 [00:00<00:00, 47531.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to object-1 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 4618.04it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install -q roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"Awfsxtk5OsUIpnuHP03u\")\n",
    "project = rf.workspace(\"signlanguage-mrsyj\").project(\"object-guksz\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIhqSj1iCkau",
    "outputId": "70819e47-d961-4f9e-aa66-e2fded409a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd /content/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWPaSEVGCsC5",
    "outputId": "55b1541a-3878-4dfc-97bf-73d789419440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:\n",
      "- boat\n",
      "- parachute\n",
      "- person\n",
      "nc: 3\n",
      "roboflow:\n",
      "  license: CC BY 4.0\n",
      "  project: object-guksz\n",
      "  url: https://universe.roboflow.com/signlanguage-mrsyj/object-guksz/dataset/1\n",
      "  version: 1\n",
      "  workspace: signlanguage-mrsyj\n",
      "test: ../test/images\n",
      "train: object-1/train/images\n",
      "val: object-1/valid/images\n"
     ]
    }
   ],
   "source": [
    "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
    "%cat {dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U6gT50vQCwZ6"
   },
   "outputs": [],
   "source": [
    "# define number of classes based on YAML\n",
    "import yaml\n",
    "with open(dataset.location + \"/data.yaml\", 'r') as stream:\n",
    "    num_classes = str(yaml.safe_load(stream)['nc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STZ1J4spCzig",
    "outputId": "3bd28a48-d69b-4955-c1db-7ddca1c0bf06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license\n",
      "\n",
      "# Parameters\n",
      "nc: 80  # number of classes\n",
      "depth_multiple: 0.33  # model depth multiple\n",
      "width_multiple: 0.50  # layer channel multiple\n",
      "anchors:\n",
      "  - [10,13, 16,30, 33,23]  # P3/8\n",
      "  - [30,61, 62,45, 59,119]  # P4/16\n",
      "  - [116,90, 156,198, 373,326]  # P5/32\n",
      "\n",
      "# YOLOv5 v6.0 backbone\n",
      "backbone:\n",
      "  # [from, number, module, args]\n",
      "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
      "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
      "   [-1, 3, C3, [128]],\n",
      "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
      "   [-1, 6, C3, [256]],\n",
      "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
      "   [-1, 9, C3, [512]],\n",
      "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
      "   [-1, 3, C3, [1024]],\n",
      "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
      "  ]\n",
      "\n",
      "# YOLOv5 v6.0 head\n",
      "head:\n",
      "  [[-1, 1, Conv, [512, 1, 1]],\n",
      "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
      "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
      "   [-1, 3, C3, [512, False]],  # 13\n",
      "\n",
      "   [-1, 1, Conv, [256, 1, 1]],\n",
      "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
      "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
      "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
      "\n",
      "   [-1, 1, Conv, [256, 3, 2]],\n",
      "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
      "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
      "\n",
      "   [-1, 1, Conv, [512, 3, 2]],\n",
      "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
      "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
      "\n",
      "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
      "  ]\n"
     ]
    }
   ],
   "source": [
    "#this is the model configuration we will use for our tutorial\n",
    "%cat /content/yolov5/models/yolov5s.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKJFYus-Czki",
    "outputId": "5f0ca6e7-560c-4f1e-eb8d-6eecf80a42cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/yolov5\n",
      "2024-11-07 17:26:18.018891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-07 17:26:18.051925: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-07 17:26:18.061509: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 17:26:18.088791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-07 17:26:20.358009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=./models/yolov5s.yaml, data=/content/yolov5/object-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5s_results, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mâš ï¸ YOLOv5 is out of date by 307 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 ğŸš€ v7.0-72-g064365d8 Python-3.10.12 torch-2.5.0+cu121 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ğŸš€ in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ğŸš€ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "/content/yolov5/train.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(weights, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 342/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/object-1/train/labels.cache... 53 images, 1 backgrounds, 0 corrupt: 100% 53/53 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 53/53 [00:00<00:00, 173.08it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/object-1/valid/labels.cache... 15 images, 0 backgrounds, 0 corrupt: 100% 15/15 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 15/15 [00:00<00:00, 96.73it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.15 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Plotting labels to runs/train/yolov5s_results2/labels.jpg... \n",
      "/content/yolov5/train.py:253: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/yolov5s_results2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0% 0/4 [00:00<?, ?it/s]/content/yolov5/train.py:308: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1199    0.03542    0.04213         59        640:   0% 0/4 [00:40<?, ?it/s]Exception in thread Thread-12 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 290, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 91, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height (WARNING: deprecated) in 9.2.0\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "        0/0         0G     0.1199    0.03542    0.04213         59        640:  25% 1/4 [00:46<02:19, 46.42s/it]/content/yolov5/train.py:308: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G     0.1211    0.03346    0.04172         47        640:  50% 2/4 [01:19<01:16, 38.48s/it]/content/yolov5/train.py:308: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "Exception in thread Thread-13 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 290, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 91, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height (WARNING: deprecated) in 9.2.0\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "        0/0         0G     0.1212    0.03361    0.04177         56        640:  75% 3/4 [01:50<00:34, 34.94s/it]/content/yolov5/train.py:308: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "Exception in thread Thread-14 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 290, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 91, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height (WARNING: deprecated) in 9.2.0\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "        0/0         0G     0.1213    0.03522     0.0416         26        640: 100% 4/4 [01:58<00:00, 29.51s/it]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/1 [00:00<?, ?it/s]WARNING âš ï¸ NMS time limit 1.250s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:07<00:00,  7.90s/it]\n",
      "                   all         15         25    0.00362       0.19     0.0324      0.006\n",
      "\n",
      "1 epochs completed in 0.035 hours.\n",
      "/content/yolov5/utils/general.py:999: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(f, map_location=torch.device('cpu'))\n",
      "Optimizer stripped from runs/train/yolov5s_results2/weights/last.pt, 14.5MB\n",
      "Optimizer stripped from runs/train/yolov5s_results2/weights/best.pt, 14.5MB\n",
      "\n",
      "Validating runs/train/yolov5s_results2/weights/best.pt...\n",
      "/content/yolov5/models/experimental.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/1 [00:00<?, ?it/s]WARNING âš ï¸ NMS time limit 1.250s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:10<00:00, 10.30s/it]\n",
      "                   all         15         25    0.00361      0.298     0.0142    0.00248\n",
      "                  boat         15          4    0.00526       0.75     0.0378    0.00696\n",
      "             parachute         15          7          0          0          0          0\n",
      "                person         15         14    0.00557      0.143    0.00482   0.000482\n",
      "Exception in thread Thread-15 (plot_images):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 290, in plot_images\n",
      "    annotator.box_label(box, label, color=color)\n",
      "  File \"/content/yolov5/utils/plots.py\", line 91, in box_label\n",
      "    w, h = self.font.getsize(label)  # text width, height (WARNING: deprecated) in 9.2.0\n",
      "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
      "Results saved to \u001b[1mruns/train/yolov5s_results2\u001b[0m\n",
      "CPU times: user 1.97 s, sys: 211 ms, total: 2.18 s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "# train yolov5s on custom data for 100 epochs\n",
    "# time its performance\n",
    "%%time\n",
    "%cd /content/yolov5/\n",
    "!python train.py --img 640 --batch 16 --epochs 1 --data {dataset.location}/data.yaml --cfg ./models/yolov5s.yaml --weights yolov5s.pt --name yolov5s_results  --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53pPszYIEo0T",
    "outputId": "7744863c-09cf-4daf-9fcd-83ffd493ce8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/yolov5\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/yolov5s_results2/weights/best.pt'], source=/content/yolov5/object-1/test/images/adv8_jpg.rf.105ef10c3272a1601f649bee59eb81cd.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ğŸš€ v7.0-72-g064365d8 Python-3.10.12 torch-2.5.0+cu121 CPU\n",
      "\n",
      "/content/yolov5/models/experimental.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/1 /content/yolov5/object-1/test/images/adv8_jpg.rf.105ef10c3272a1601f649bee59eb81cd.jpg: 640x640 (no detections), 605.4ms\n",
      "Speed: 5.6ms pre-process, 605.4ms inference, 0.7ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n",
      "CPU times: user 93.3 ms, sys: 15.1 ms, total: 108 ms\n",
      "Wall time: 9.16 s\n"
     ]
    }
   ],
   "source": [
    "# train yolov5s on custom data for 100 epochs\n",
    "# time its performance\n",
    "%%time\n",
    "%cd /content/yolov5/\n",
    "!python detect.py --img 640 --weights runs/train/yolov5s_results2/weights/best.pt --img 640 --conf 0.4 --source {dataset.location}/test/images/adv8_jpg.rf.105ef10c3272a1601f649bee59eb81cd.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsjV2iefDYUD"
   },
   "source": [
    "%cd /content/yolov5/\n",
    "!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 640 --conf 0.4 --source Cash-Counter-10/test/images/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
