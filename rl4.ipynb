{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function (V):\n",
      "[[48.45850102 54.95389102 62.17099102 70.18999102]\n",
      " [54.95389102 62.17099102 70.18999102 79.09999102]\n",
      " [62.17099102 70.18999102 79.09999102 88.99999102]\n",
      " [70.18999102 79.09999102 88.99999102 99.99999102]]\n",
      "\n",
      "Optimal Policy (actions corresponding to indices):\n",
      "[['down' 'right' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' 'right' 'right' 'up']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DiscountGridEnv:\n",
    "    def __init__(self, grid_size=(4, 4), goal_state=(3, 3), discount_factor=0.9, obstacle_states=[], goal_reward=10, step_penalty=-1):\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_state = goal_state\n",
    "        self.discount_factor = discount_factor\n",
    "        self.obstacle_states = obstacle_states\n",
    "        self.goal_reward = goal_reward\n",
    "        self.step_penalty = step_penalty\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.n_states = grid_size[0] * grid_size[1]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.transitions, self.rewards = self.build_environment()\n",
    "\n",
    "    def build_environment(self):\n",
    "        transition_prob = np.zeros((self.n_states, self.n_actions, self.n_states))\n",
    "        rewards = np.full((self.n_states, self.n_actions), self.step_penalty)\n",
    "\n",
    "        def state_to_index(row, col):\n",
    "            return row * self.grid_size[1] + col\n",
    "\n",
    "        def is_valid_state(row, col):\n",
    "            return (0 <= row < self.grid_size[0]) and (0 <= col < self.grid_size[1]) and (row, col) not in self.obstacle_states\n",
    "\n",
    "        for row in range(self.grid_size[0]):\n",
    "            for col in range(self.grid_size[1]):\n",
    "                state = state_to_index(row, col)\n",
    "                for action_idx, action in enumerate(self.actions):\n",
    "                    if (row, col) == self.goal_state:\n",
    "                        rewards[state, action_idx] = self.goal_reward\n",
    "                        transition_prob[state, action_idx, state] = 1.0\n",
    "                        continue\n",
    "                    next_row, next_col = row, col\n",
    "                    if action == 'up' and is_valid_state(row - 1, col):\n",
    "                        next_row = row - 1\n",
    "                    elif action == 'down' and is_valid_state(row + 1, col):\n",
    "                        next_row = row + 1\n",
    "                    elif action == 'left' and is_valid_state(row, col - 1):\n",
    "                        next_col = col - 1\n",
    "                    elif action == 'right' and is_valid_state(row, col + 1):\n",
    "                        next_col = col + 1\n",
    "                    next_state = state_to_index(next_row, next_col)\n",
    "                    transition_prob[state, action_idx, next_state] = 1.0\n",
    "        return transition_prob, rewards\n",
    "\n",
    "    def state_index_to_coordinates(self, state_index):\n",
    "        row = state_index // self.grid_size[1]\n",
    "        col = state_index % self.grid_size[1]\n",
    "        return row, col\n",
    "\n",
    "def value_iteration_discount_grid(env, gamma=0.9, theta=1e-6):\n",
    "    V = np.zeros(env.n_states)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.n_states):\n",
    "            v = V[s]\n",
    "            Q_sa = np.zeros(env.n_actions)\n",
    "            for a in range(env.n_actions):\n",
    "                Q_sa[a] = sum([env.transitions[s, a, s_prime] * (env.rewards[s, a] + gamma * V[s_prime])\n",
    "                               for s_prime in range(env.n_states)])\n",
    "            V[s] = max(Q_sa)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros(env.n_states, dtype=int)\n",
    "    for s in range(env.n_states):\n",
    "        Q_sa = np.zeros(env.n_actions)\n",
    "        for a in range(env.n_actions):\n",
    "            Q_sa[a] = sum([env.transitions[s, a, s_prime] * (env.rewards[s, a] + gamma * V[s_prime])\n",
    "                           for s_prime in range(env.n_states)])\n",
    "        policy[s] = np.argmax(Q_sa)\n",
    "    return V, policy\n",
    "\n",
    "env = DiscountGridEnv(grid_size=(4, 4), goal_state=(3, 3), discount_factor=0.9, obstacle_states=[(1, 1)], goal_reward=10, step_penalty=-1)\n",
    "V, policy = value_iteration_discount_grid(env, gamma=0.9)\n",
    "print(\"Optimal Value Function (V):\")\n",
    "print(V.reshape(env.grid_size))\n",
    "print(\"\\nOptimal Policy (actions corresponding to indices):\")\n",
    "policy_grid = np.array(env.actions)[policy].reshape(env.grid_size)\n",
    "print(policy_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
