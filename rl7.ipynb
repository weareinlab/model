{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the learned policy...\n",
      "Step 1: State (0, 0), Action up, Reward -1\n",
      "Step 2: State (0, 0), Action up, Reward -1\n",
      "Step 3: State (0, 0), Action up, Reward -1\n",
      "Step 4: State (0, 0), Action up, Reward -1\n",
      "Step 5: State (0, 0), Action up, Reward -1\n",
      "Step 6: State (0, 0), Action up, Reward -1\n",
      "Step 7: State (0, 0), Action up, Reward -1\n",
      "Step 8: State (0, 0), Action up, Reward -1\n",
      "Step 9: State (0, 0), Action up, Reward -1\n",
      "Step 10: State (0, 0), Action up, Reward -1\n",
      "Step 11: State (0, 0), Action up, Reward -1\n",
      "Step 12: State (0, 0), Action up, Reward -1\n",
      "Step 13: State (0, 0), Action up, Reward -1\n",
      "Step 14: State (0, 0), Action down, Reward -1\n",
      "Step 15: State (1, 0), Action up, Reward -1\n",
      "Step 16: State (0, 0), Action up, Reward -1\n",
      "Step 17: State (0, 0), Action up, Reward -1\n",
      "Step 18: State (0, 0), Action up, Reward -1\n",
      "Step 19: State (0, 0), Action down, Reward -1\n",
      "Step 20: State (1, 0), Action up, Reward -1\n",
      "Step 21: State (0, 0), Action up, Reward -1\n",
      "Step 22: State (0, 0), Action up, Reward -1\n",
      "Step 23: State (0, 0), Action up, Reward -1\n",
      "Step 24: State (0, 0), Action up, Reward -1\n",
      "Step 25: State (0, 0), Action up, Reward -1\n",
      "Step 26: State (0, 0), Action up, Reward -1\n",
      "Step 27: State (0, 0), Action up, Reward -1\n",
      "Step 28: State (0, 0), Action up, Reward -1\n",
      "Step 29: State (0, 0), Action up, Reward -1\n",
      "Step 30: State (0, 0), Action up, Reward -1\n",
      "Step 31: State (0, 0), Action up, Reward -1\n",
      "Step 32: State (0, 0), Action up, Reward -1\n",
      "Step 33: State (0, 0), Action up, Reward -1\n",
      "Step 34: State (0, 0), Action up, Reward -1\n",
      "Step 35: State (0, 0), Action up, Reward -1\n",
      "Step 36: State (0, 0), Action up, Reward -1\n",
      "Step 37: State (0, 0), Action up, Reward -1\n",
      "Step 38: State (0, 0), Action up, Reward -1\n",
      "Step 39: State (0, 0), Action down, Reward -1\n",
      "Step 40: State (1, 0), Action up, Reward -1\n",
      "Step 41: State (0, 0), Action up, Reward -1\n",
      "Step 42: State (0, 0), Action up, Reward -1\n",
      "Step 43: State (0, 0), Action up, Reward -1\n",
      "Step 44: State (0, 0), Action up, Reward -1\n",
      "Step 45: State (0, 0), Action up, Reward -1\n",
      "Step 46: State (0, 0), Action up, Reward -1\n",
      "Step 47: State (0, 0), Action up, Reward -1\n",
      "Step 48: State (0, 0), Action up, Reward -1\n",
      "Step 49: State (0, 0), Action up, Reward -1\n",
      "Step 50: State (0, 0), Action up, Reward -1\n",
      "Step 51: State (0, 0), Action up, Reward -1\n",
      "Step 52: State (0, 0), Action up, Reward -1\n",
      "Step 53: State (0, 0), Action up, Reward -1\n",
      "Step 54: State (0, 0), Action up, Reward -1\n",
      "Step 55: State (0, 0), Action up, Reward -1\n",
      "Step 56: State (0, 0), Action up, Reward -1\n",
      "Step 57: State (0, 0), Action up, Reward -1\n",
      "Step 58: State (0, 0), Action up, Reward -1\n",
      "Step 59: State (0, 0), Action up, Reward -1\n",
      "Step 60: State (0, 0), Action up, Reward -1\n",
      "Step 61: State (0, 0), Action up, Reward -1\n",
      "Step 62: State (0, 0), Action up, Reward -1\n",
      "Step 63: State (0, 0), Action up, Reward -1\n",
      "Step 64: State (0, 0), Action up, Reward -1\n",
      "Step 65: State (0, 0), Action up, Reward -1\n",
      "Step 66: State (0, 0), Action up, Reward -1\n",
      "Step 67: State (0, 0), Action up, Reward -1\n",
      "Step 68: State (0, 0), Action up, Reward -1\n",
      "Step 69: State (0, 0), Action up, Reward -1\n",
      "Step 70: State (0, 0), Action up, Reward -1\n",
      "Step 71: State (0, 0), Action up, Reward -1\n",
      "Step 72: State (0, 0), Action up, Reward -1\n",
      "Step 73: State (0, 0), Action up, Reward -1\n",
      "Step 74: State (0, 0), Action up, Reward -1\n",
      "Step 75: State (0, 0), Action up, Reward -1\n",
      "Step 76: State (0, 0), Action up, Reward -1\n",
      "Step 77: State (0, 0), Action up, Reward -1\n",
      "Step 78: State (0, 0), Action up, Reward -1\n",
      "Step 79: State (0, 0), Action up, Reward -1\n",
      "Step 80: State (0, 0), Action up, Reward -1\n",
      "Step 81: State (0, 0), Action up, Reward -1\n",
      "Step 82: State (0, 0), Action up, Reward -1\n",
      "Step 83: State (0, 0), Action right, Reward -1\n",
      "Step 84: State (0, 1), Action up, Reward -1\n",
      "Step 85: State (0, 1), Action up, Reward -1\n",
      "Step 86: State (0, 1), Action up, Reward -1\n",
      "Step 87: State (0, 1), Action up, Reward -1\n",
      "Step 88: State (0, 1), Action up, Reward -1\n",
      "Step 89: State (0, 1), Action up, Reward -1\n",
      "Step 90: State (0, 1), Action up, Reward -1\n",
      "Step 91: State (0, 1), Action up, Reward -1\n",
      "Step 92: State (0, 1), Action up, Reward -1\n",
      "Step 93: State (0, 1), Action up, Reward -1\n",
      "Step 94: State (0, 1), Action up, Reward -1\n",
      "Step 95: State (0, 1), Action right, Reward -1\n",
      "Step 96: State (0, 2), Action down, Reward -1\n",
      "Step 97: State (1, 2), Action up, Reward -1\n",
      "Step 98: State (0, 2), Action up, Reward -1\n",
      "Step 99: State (0, 2), Action up, Reward -1\n",
      "Step 100: State (0, 2), Action up, Reward -1\n",
      "Step 101: State (0, 2), Action up, Reward -1\n",
      "Step 102: State (0, 2), Action up, Reward -1\n",
      "Step 103: State (0, 2), Action up, Reward -1\n",
      "Step 104: State (0, 2), Action up, Reward -1\n",
      "Step 105: State (0, 2), Action up, Reward -1\n",
      "Step 106: State (0, 2), Action up, Reward -1\n",
      "Step 107: State (0, 2), Action up, Reward -1\n",
      "Step 108: State (0, 2), Action up, Reward -1\n",
      "Step 109: State (0, 2), Action up, Reward -1\n",
      "Step 110: State (0, 2), Action up, Reward -1\n",
      "Step 111: State (0, 2), Action up, Reward -1\n",
      "Step 112: State (0, 2), Action up, Reward -1\n",
      "Step 113: State (0, 2), Action up, Reward -1\n",
      "Step 114: State (0, 2), Action up, Reward -1\n",
      "Step 115: State (0, 2), Action up, Reward -1\n",
      "Step 116: State (0, 2), Action down, Reward -1\n",
      "Step 117: State (1, 2), Action up, Reward -1\n",
      "Step 118: State (0, 2), Action up, Reward -1\n",
      "Step 119: State (0, 2), Action up, Reward -1\n",
      "Step 120: State (0, 2), Action up, Reward -1\n",
      "Step 121: State (0, 2), Action up, Reward -1\n",
      "Step 122: State (0, 2), Action up, Reward -1\n",
      "Step 123: State (0, 2), Action up, Reward -1\n",
      "Step 124: State (0, 2), Action up, Reward -1\n",
      "Step 125: State (0, 2), Action up, Reward -1\n",
      "Step 126: State (0, 2), Action up, Reward -1\n",
      "Step 127: State (0, 2), Action up, Reward -1\n",
      "Step 128: State (0, 2), Action up, Reward -1\n",
      "Step 129: State (0, 2), Action up, Reward -1\n",
      "Step 130: State (0, 2), Action up, Reward -1\n",
      "Step 131: State (0, 2), Action up, Reward -1\n",
      "Step 132: State (0, 2), Action up, Reward -1\n",
      "Step 133: State (0, 2), Action up, Reward -1\n",
      "Step 134: State (0, 2), Action up, Reward -1\n",
      "Step 135: State (0, 2), Action up, Reward -1\n",
      "Step 136: State (0, 2), Action up, Reward -1\n",
      "Step 137: State (0, 2), Action up, Reward -1\n",
      "Step 138: State (0, 2), Action up, Reward -1\n",
      "Step 139: State (0, 2), Action up, Reward -1\n",
      "Step 140: State (0, 2), Action up, Reward -1\n",
      "Step 141: State (0, 2), Action up, Reward -1\n",
      "Step 142: State (0, 2), Action up, Reward -1\n",
      "Step 143: State (0, 2), Action up, Reward -1\n",
      "Step 144: State (0, 2), Action up, Reward -1\n",
      "Step 145: State (0, 2), Action up, Reward -1\n",
      "Step 146: State (0, 2), Action left, Reward -1\n",
      "Step 147: State (0, 1), Action up, Reward -1\n",
      "Step 148: State (0, 1), Action up, Reward -1\n",
      "Step 149: State (0, 1), Action up, Reward -1\n",
      "Step 150: State (0, 1), Action up, Reward -1\n",
      "Step 151: State (0, 1), Action up, Reward -1\n",
      "Step 152: State (0, 1), Action up, Reward -1\n",
      "Step 153: State (0, 1), Action up, Reward -1\n",
      "Step 154: State (0, 1), Action up, Reward -1\n",
      "Step 155: State (0, 1), Action up, Reward -1\n",
      "Step 156: State (0, 1), Action up, Reward -1\n",
      "Step 157: State (0, 1), Action up, Reward -1\n",
      "Step 158: State (0, 1), Action up, Reward -1\n",
      "Step 159: State (0, 1), Action up, Reward -1\n",
      "Step 160: State (0, 1), Action left, Reward -1\n",
      "Step 161: State (0, 0), Action up, Reward -1\n",
      "Step 162: State (0, 0), Action up, Reward -1\n",
      "Step 163: State (0, 0), Action up, Reward -1\n",
      "Step 164: State (0, 0), Action up, Reward -1\n",
      "Step 165: State (0, 0), Action up, Reward -1\n",
      "Step 166: State (0, 0), Action up, Reward -1\n",
      "Step 167: State (0, 0), Action up, Reward -1\n",
      "Step 168: State (0, 0), Action up, Reward -1\n",
      "Step 169: State (0, 0), Action up, Reward -1\n",
      "Step 170: State (0, 0), Action right, Reward -1\n",
      "Step 171: State (0, 1), Action up, Reward -1\n",
      "Step 172: State (0, 1), Action up, Reward -1\n",
      "Step 173: State (0, 1), Action down, Reward -10\n",
      "Total Reward: -182, Steps Taken: 173\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class GridworldEnv:\n",
    "    def __init__(self, grid_size=(4, 4), goal_state=(3, 3), trap_states=[], goal_reward=10, trap_penalty=-10, step_penalty=-1):\n",
    "        self.grid_size = grid_size\n",
    "        self.goal_state = goal_state\n",
    "        self.trap_states = trap_states\n",
    "        self.goal_reward = goal_reward\n",
    "        self.trap_penalty = trap_penalty\n",
    "        self.step_penalty = step_penalty\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.n_actions = len(self.actions)\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = (0, 0)\n",
    "        return self.agent_position\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = self.agent_position\n",
    "        if action == 0:  \n",
    "            next_position = (max(row - 1, 0), col)\n",
    "        elif action == 1: \n",
    "            next_position = (min(row + 1, self.grid_size[0] - 1), col)\n",
    "        elif action == 2:\n",
    "            next_position = (row, max(col - 1, 0))\n",
    "        elif action == 3:  \n",
    "            next_position = (row, min(col + 1, self.grid_size[1] - 1))\n",
    "        reward = self.step_penalty\n",
    "        done = False\n",
    "        if next_position == self.goal_state:\n",
    "            reward = self.goal_reward\n",
    "            done = True\n",
    "        elif next_position in self.trap_states:\n",
    "            reward = self.trap_penalty\n",
    "            done = True\n",
    "        self.agent_position = next_position\n",
    "        return next_position, reward, done\n",
    "\n",
    "    def action_space(self):\n",
    "        return self.n_actions\n",
    "\n",
    "class ApproximateQLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, feature_dim=8):\n",
    "        self.env = env\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.epsilon = epsilon\n",
    "        self.feature_dim = feature_dim  \n",
    "        self.weights = np.zeros(self.feature_dim)  \n",
    "\n",
    "    def extract_features(self, state, action):\n",
    "        row, col = state\n",
    "        feature_vector = np.zeros(self.feature_dim)\n",
    "        feature_vector[0] = row\n",
    "        feature_vector[1] = col\n",
    "        feature_vector[2] = row * col\n",
    "        feature_vector[3] = action\n",
    "        feature_vector[4] = row - col\n",
    "        feature_vector[5] = row + col\n",
    "        feature_vector[6] = 1 if state == self.env.goal_state else 0\n",
    "        feature_vector[7] = 1 if state in self.env.trap_states else 0\n",
    "        return feature_vector\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        features = self.extract_features(state, action)\n",
    "        return np.dot(self.weights, features)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(range(self.env.n_actions))\n",
    "        else:\n",
    "            q_values = [self.q_value(state, a) for a in range(self.env.n_actions)]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q_values = [self.q_value(next_state, a) for a in range(self.env.n_actions)]\n",
    "            target = reward + self.gamma * np.max(next_q_values)\n",
    "        current_q = self.q_value(state, action)\n",
    "        td_error = target - current_q\n",
    "        features = self.extract_features(state, action)\n",
    "        self.weights += self.alpha * td_error * features\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.update(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "\n",
    "    def test(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        print(\"Testing the learned policy...\")\n",
    "        while not done:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            print(f\"Step {steps}: State {state}, Action {self.env.actions[action]}, Reward {reward}\")\n",
    "            state = next_state\n",
    "        print(f\"Total Reward: {total_reward}, Steps Taken: {steps}\")\n",
    "\n",
    "env = GridworldEnv(grid_size=(4, 4), goal_state=(3, 3), trap_states=[(1, 1)], goal_reward=10, trap_penalty=-10)\n",
    "agent = ApproximateQLearningAgent(env)\n",
    "agent.train(episodes=1000)\n",
    "agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
